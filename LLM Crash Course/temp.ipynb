{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Lessons about Natural Lanuage Processing\n",
    "\n",
    "- Word embeddings work by vectorizing all the words so that the ones that are the most similar to eachother are the closest to eachother\n",
    "- Way we can define embedding using pytorch is through `nn.Embeddings` and defining it in the constructor of your model similar to what we did before with CNN's\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Here's a sample model build out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "\n",
    "\n",
    "class SentimentAnalysisModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        self.embeddings = nn.Embedding(vocab_size, 16) \n",
    "        '''\n",
    "            The idea by setting the words into a 16 dimension vector. We are essentially extracting more features from the data so that we when\n",
    "            we consolidate and train the data we are able to pick on more relationships between the data to get better results. \n",
    "        '''\n",
    "\n",
    "        self.linear = nn.linear(in_features = 16, out_features= 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        word_embeddings = self.embeddings(x)\n",
    "        averaged = torch.mean(word_embeddings, dim = 1)\n",
    "\n",
    "        return self.sigmoid(self.linear(averaged)) # shape will be 2 by 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is dropout?\n",
    "\n",
    "\n",
    "Dropout is basically to combat overfitting which is when the model overfits due to having too much data whether thats in the form of having too many layers, too many nodes, etc which causes the model to be less accurate because it is memorizing too many irrelevant details.\n",
    "\n",
    "\n",
    "Implementing dropout is simple, its literally jsut doing adding a `nn.Dropout()` with some probabiltiy as its parameter (p =)\n",
    "\n",
    "What this does is that for every node that is below or equal to that probability, the activation is 0 meaning that all the model only picks up on the details that you want it to. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Digit Classifier Model ( with sample data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class DigitClassifierModel01(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features= 784, out_features= 512),\n",
    "            nn.Relu(),\n",
    "            nn.Dropout(p=.2),\n",
    "            nn.Linear(in_features= 512, out_features= 10),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training and Testing\n",
    "\n",
    "\n",
    "sample_data_test\n",
    "\n",
    "sample_data_train \n",
    "\n",
    "#assume variables above represent training and testing datasets that fully formatted and on the same device for the model\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "model01 = DigitClassifierModel01()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(params= model01.parameters(), lr= 0.01)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for X,y in enumerate(sample_data_train):\n",
    "\n",
    "        model01.train()\n",
    "\n",
    "        # forward pass\n",
    "        y_predictions = model01(X)\n",
    "\n",
    "        #calulcate loss\n",
    "\n",
    "        loss = loss_fn(y_predictions, y)\n",
    "\n",
    "        # optimizer zero grad\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #backpropogation \n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        #forward step \n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for X,y in enumerate(sample_data_test):\n",
    "\n",
    "        model01.train()\n",
    "\n",
    "        # forward pass\n",
    "        y_predictions = model01(X)\n",
    "\n",
    "        #calulcate loss\n",
    "\n",
    "        loss = loss_fn(y_predictions, y)\n",
    "\n",
    "        # optimizer zero grad\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #backpropogation \n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        #forward step \n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
